import os
os.environ["TRANSFORMERS_NO_TF"] = "1"   # Transformersê°€ TensorFlow ë¶ˆëŸ¬ì˜¤ì§€ ì•Šê²Œ
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2" # TF ë¡œê·¸ ì¤„ì´ê¸°

import sys
import tempfile
import requests
import speech_recognition as sr
import contextlib
import pygame
import noisereduce as nr
import soundfile as sf
import torch, cv2
import time
import numpy as np
import tensorflow as tf
tf.config.set_visible_devices([], 'GPU')   # TFê°€ ì–´ë–¤ GPUë„ ë³´ì§€ ëª»í•˜ê²Œ

from transformers import pipeline
from keras.models import load_model
from gtts import gTTS


# ========== ì„¤ì • ==========
USE_GPU = torch.cuda.is_available()
DEVICE = 0 if USE_GPU else -1
WAKE_WORDS = ["í—¬ë¡œ", "hi", "í•˜ì´", "ì•ˆë…•"]
EXIT_WORDS = ["ì—†ì–´", "ëì–´", "ì•„ë‹ˆ"]
MUSIC_PATH = "/home/naseungwon/reachy_mini/no-copyright-music-1.mp3"

# [Keras ê°ì • ë¶„ë¥˜ ëª¨ë¸ & ì–¼êµ´ ê²€ì¶œê¸° ê²½ë¡œ]
EMOTION_MODEL_PATH = "/home/naseungwon/reachy_mini/face_recognition/emotion_model.hdf5"  
FACE_CASCADE_PATH = "/home/naseungwon/reachy_mini/face_recognition/haarcascade_frontalface_default.xml"

# ë¼ë²¨ (ëª¨ë¸ í•™ìŠµ ë¼ë²¨ì— ë§ì¶° ìˆ˜ì • ê°€ëŠ¥)
EMOTION_LABELS = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']

# ëª¨ë¸ ë¡œë“œ
emotion_model = load_model(EMOTION_MODEL_PATH, compile=False)
face_cascade = cv2.CascadeClassifier(FACE_CASCADE_PATH)

# ========== ì´ˆê¸°í™” ==========
pygame.mixer.init()
pygame.mixer.set_num_channels(8)  # ì±„ë„ ì—¬ìœ  í™•ë³´
TTS_CH = pygame.mixer.Channel(7)  # TTS ì „ìš© ì±„ë„ í•˜ë‚˜ ì¡ê¸°

print("ğŸ§  ë¦¬ì¹˜ ë¯¸ë‹ˆ ë¡œë”© ì¤‘...")
stt_pipeline = pipeline("automatic-speech-recognition",
                         model="openai/whisper-large-v3",
                           device=DEVICE, 
                           framework="pt",
                           torch_dtype=torch.float16 if USE_GPU else torch.float32,
                           )
llm = pipeline(
    "text2text-generation",
    model="google/flan-t5-base",
    device=-1,              # CPU ì‚¬ìš©
    framework="pt",         # PyTorch ê°•ì œ
    torch_dtype=torch.float32
)
# ========== ìœ í‹¸ í•¨ìˆ˜ ==========
@contextlib.contextmanager
def suppress_stderr():
    with open(os.devnull, 'w') as fnull:
        stderr = sys.stderr
        sys.stderr = fnull
        try:
            yield
        finally:
            sys.stderr = stderr

def _wait_tts_quiet(extra_ms=250):
    """TTSê°€ ëë‚  ë•Œê¹Œì§€ ëŒ€ê¸° (ì—ì½” ë°©ì§€) + ì—¬ìœ """
    try:
        while TTS_CH.get_busy():
            time.sleep(0.05)
    except Exception:
        pass
    time.sleep(extra_ms / 1000.0)

class _MusicDucker:
    def __init__(self, vol=0.25):
        self.vol = vol
        self.prev = None
    def __enter__(self):
        try:
            self.prev = pygame.mixer.music.get_volume()
            pygame.mixer.music.set_volume(self.vol)
        except Exception:
            pass
    def __exit__(self, exc_type, exc, tb):
        try:
            if self.prev is not None:
                pygame.mixer.music.set_volume(self.prev)
        except Exception:
            pass


# ========== ìŒì„± ì…ë ¥ ==========
def listen_audio(timeout=10, phrase_time_limit=5, filename="input.wav"):
    _wait_tts_quiet()  # ğŸ‘ˆ TTS ëë‚  ë•Œê¹Œì§€ ëŒ€ê¸° (ê°€ì¥ ì¤‘ìš”!)
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("ğŸ¤ ìŒì„±ì„ ë“£ê³  ìˆì–´ìš”...")
        try:
            recognizer.adjust_for_ambient_noise(source, duration=0.3)
        except Exception:
            pass
        with suppress_stderr():
            audio = recognizer.listen(source, timeout=timeout, phrase_time_limit=phrase_time_limit)
            with open(filename, "wb") as f:
                f.write(audio.get_wav_data())
            print(f"âœ… ì €ì¥ ì™„ë£Œ: {filename}")
    return filename

def clean_audio(input_path="input.wav", output_path="cleaned.wav"):
    try:
        data, rate = sf.read(input_path)
        reduced = nr.reduce_noise(y=data, sr=rate)
        sf.write(output_path, reduced, rate)
        print(f"ğŸ”‡ ë…¸ì´ì¦ˆ ì œê±° ì™„ë£Œ â†’ {output_path}")
        return output_path
    except Exception as e:
        print("âŒ ë…¸ì´ì¦ˆ ì œê±° ì‹¤íŒ¨:", e)
        return input_path

def transcribe_audio(filename="cleaned.wav"):
    result = stt_pipeline(filename)
    print("ğŸ“ ì¸ì‹ëœ í…ìŠ¤íŠ¸:", result['text'])
    return result['text'].strip()

# ========== í…ìŠ¤íŠ¸ ì‘ë‹µ ìƒì„± ==========
def generate_response(text):
    prompt = f"ì§ˆë¬¸: {text.strip()}\nëŒ€ë‹µ:"
    result = llm(prompt, max_new_tokens=100)
    response = result[0]["generated_text"].strip()
    return response if response else "ì£„ì†¡í•´ìš”, ì˜ ì´í•´í•˜ì§€ ëª»í–ˆì–´ìš”."

# ========== ìŒì„± ì¶œë ¥ ==========
def speak(text, lang='ko', blocking=True):
    if not text.strip():
        print("âš ï¸ ìŒì„±ìœ¼ë¡œ ì¶œë ¥í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    print(f"ğŸ—£ ì‘ë‹µ: {text}")
    tts = gTTS(text=text, lang=lang)
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as fp:
        tts.save(fp.name)
        snd = pygame.mixer.Sound(fp.name)

    # ìŒì•…ì„ ì‚´ì§ ì¤„ì´ê³ (TTS ì„ ëª…ë„â†‘) TTS ì „ìš© ì±„ë„ì—ì„œ ì¬ìƒ
    with _MusicDucker(vol=0.25):
        TTS_CH.play(snd)
        if blocking:
            while TTS_CH.get_busy():
                time.sleep(0.05)

def speak_nonblocking(text, lang='ko'):
    tts = gTTS(text=text, lang=lang)
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as fp:
        tts.save(fp.name)
        sound = pygame.mixer.Sound(fp.name)
        ch = pygame.mixer.find_channel()
        if ch:
            ch.play(sound)

# ========== ìŒì•… ì¬ìƒ ==========
def play_music():
    if os.path.exists(MUSIC_PATH):
        pygame.mixer.music.load(MUSIC_PATH)
        pygame.mixer.music.play()
        print("ğŸµ ìŒì•… ì¬ìƒ ì¤‘...")
    else:
        speak("ì£„ì†¡í•´ìš”, ìŒì•… íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ìš”.")

def stop_music():
    if pygame.mixer.music.get_busy():
        pygame.mixer.music.stop()
        print("â¹ ìŒì•… êº¼ì§")

# ========== ë‚ ì”¨ ì •ë³´ ==========
def get_weather(city="Seoul"):
    API_KEY = "your_API"
    url = f"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={API_KEY}&lang=kr&units=metric"
    res = requests.get(url)
    if res.status_code == 200:
        data = res.json()
        desc = data["weather"][0]["description"]
        temp = data["main"]["temp"]
        return f"{city}ì˜ í˜„ì¬ ë‚ ì”¨ëŠ” {desc}, ê¸°ì˜¨ì€ {temp:.1f}ë„ì…ë‹ˆë‹¤."
    else:
        return "ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆì–´ìš”."

# ========== ê°ì • ì¸ì‹ (Keras) ==========
def _preprocess_face(gray_face):
    """(H, W) -> (1, 64, 64, 1), ì •ê·œí™”"""
    face = cv2.resize(gray_face, (64, 64))
    face = face.astype("float32") / 255.0
    face = np.expand_dims(face, axis=-1)  # ì±„ë„
    face = np.expand_dims(face, axis=0)   # ë°°ì¹˜
    return face

def detect_emotion_keras(timeout=5, show=False):
    """
    ì›¹ìº ì—ì„œ ê°€ì¥ í° ì–¼êµ´ 1ê°œë¥¼ ì°¾ì•„ Keras ëª¨ë¸ë¡œ ê°ì • ë¶„ë¥˜.
    ë°˜í™˜ê°’: 'happy'/'sad'/... (ì†Œë¬¸ì) ë˜ëŠ” None
    """
    try:
        cap = cv2.VideoCapture(0)
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

        if not cap.isOpened():
            print("âŒ ì›¹ìº ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None

        print("ğŸ§  ê°ì • ì¸ì‹ ì¤‘...(Keras)")
        start_time = time.time()
        detected = None

        while time.time() - start_time < timeout:
            ok, frame = cap.read()
            if not ok:
                continue

            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            faces = face_cascade.detectMultiScale(
                gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30)
            )

            if len(faces) > 0:
                # ê°€ì¥ í° ì–¼êµ´ ì„ íƒ
                x, y, w, h = max(faces, key=lambda b: b[2] * b[3])
                face_roi = gray[y:y+h, x:x+w]
                inp = _preprocess_face(face_roi)
                probs = emotion_model.predict(inp, verbose=0)[0]
                idx = int(np.argmax(probs))
                label = EMOTION_LABELS[idx]
                detected = label.lower()

                if show:
                    cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
                    cv2.putText(frame, label, (x, y-10),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)
                    cv2.imshow("Emotion (Keras)", frame)
                    if cv2.waitKey(1) & 0xFF == ord('q'):
                        break
                break
            elif show:
                cv2.imshow("Emotion (Keras)", frame)
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break

        cap.release()
        if show:
            cv2.destroyAllWindows()
        return detected
    except Exception as e:
        print(f"ğŸš¨ ê°ì • ì¸ì‹ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        return None


# ========== ë©”ì¸ ë£¨í”„ ==========
def main():
    print("ğŸ¤– 'ë¦¬ì¹˜ ë¯¸ë‹ˆ'ê°€ ì‚¬ìš©ìë¥¼ ê¸°ë‹¤ë¦¬ê³  ìˆì–´ìš” ...")

    # [1] ì›¨ì´í¬ ì›Œë“œ ëŒ€ê¸°
    while True:
        path = listen_audio()
        cleaned = clean_audio(path)
        transcript = transcribe_audio(cleaned).lower()
        if any(transcript.startswith(wake) or transcript == wake for wake in WAKE_WORDS):
            speak("ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")
            break

    # [2] ëª…ë ¹ ì²˜ë¦¬ ë£¨í”„ (ì§€ì†ì ìœ¼ë¡œ ëª…ë ¹ ìˆ˜ìš©)
    while True:
        path = listen_audio()
        cleaned = clean_audio(path)
        user_text = transcribe_audio(cleaned).lower()

        if any(bye in user_text for bye in EXIT_WORDS):
            speak("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        # ëª…ë ¹ ì²˜ë¦¬
        if "ë‚ ì”¨" in user_text:
            response = get_weather()
            speak(response)

        # ê°ì • ë¶„ì„ (Keras) ë¶„ê¸°
        elif "ê¸°ë¶„" in user_text or "ë‚´ ê¸°ë¶„" in user_text:
            speak("ê¸°ë¶„ì´ ì–´ë–¤ì§€ ë´ë“œë¦´ê²Œìš”. ì¹´ë©”ë¼ë¥¼ ì ì‹œ ë°”ë¼ë´ì£¼ì„¸ìš”.")
            emotion = detect_emotion_keras(show=False)
            if emotion in ['happy', 'sad']:
                if emotion == 'happy':
                    speak(f"ê¸°ë¶„ì´ ì¢‹ì•„ ë³´ì´ë„¤ìš”! ({emotion})")
                elif emotion == 'sad':
                    speak(f"ê¸°ë¶„ì´ ì•ˆ ì¢‹ì•„ ë³´ì´ë„¤ìš”. ë¬´ìŠ¨ ì¼ ìˆì–´ìš”? ({emotion})")
            elif emotion is not None:
                speak(f"{emotion} ê°ì •ì´ ê°ì§€ë˜ì—ˆì–´ìš”.")
            else:
                speak("ì–¼êµ´ì„ ì œëŒ€ë¡œ ê°ì§€í•˜ì§€ ëª»í–ˆì–´ìš”.")

        elif "ìŒì•…" in user_text or "ë…¸ë˜" in user_text:
            speak("ìŒì•…ì„ ì¬ìƒí• ê²Œìš”.")
            play_music()

            # ìŒì•… ì¬ìƒ ì¤‘ ë©ˆì¶¤ ê°ì§€ ë£¨í”„
            while True:
                path = listen_audio()
                cleaned = clean_audio(path)
                cmd = transcribe_audio(cleaned).lower()
                if "êº¼ì¤˜" in cmd:
                    stop_music()
                    speak("ìŒì•…ì„ ëŒê²Œìš”.")
                    break
                else:
                    speak_nonblocking("ì£„ì†¡í•´ìš”, ì˜ ëª» ì•Œì•„ ë“¤ì—ˆì–´ìš”.")
                    # ìŒì•… ê³„ì† ìœ ì§€

        else:
            response = generate_response(user_text)
            speak(response)

        # ë‹¤ìŒ ì§ˆë¬¸ ìœ ë„
        speak("ë” í•„ìš”í•œ ê±° ìˆìœ¼ì‹ ê°€ìš”?")


if __name__ == "__main__":
    main()
